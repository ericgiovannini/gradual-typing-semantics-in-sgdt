We thank the reviewers for their helpful feedback. We agree that it
would be beneficial to provide more context comparing our work to
existing approaches to gradual typing and will incorporate this
discussion into the revision.

* Reviewer A's question about choice of axioms for type precision

  Reviewer A asks why we chose the particular set of axioms for type
  precision in the paper rather than the usual equivalent set of
  axioms used in prior work where reflexivity and transitivity are
  admissible.

  The reason behind this is that in our semantics, the equivalent type
  precision derivations do not denote *equal* relations, they instead
  only denote relations that are *quasi-order-equivalent*, i.e., if
  two terms are related by one then they are related also by the other
  up to insertion of delays (see the statement of Theorem 5.12, line
  1007).

  However, because all type precision derivations in our system are
  equivalent, it is straightforward to define a translation from the
  more standard system of type and term precision into ours, so
  ultimately our graduality proofs can still be applied to the
  standard formulation.

* Reviewer A confusion on lack of transitivity of weak bisimilarity

  Reviewer A notes that the lack of transitivity of our definition of
  weak bisimilarity for L‚Ñß A is unexpected given the intuitive meaning
  of bisimilarity as being ‚Äúequivalent when ignoring the number of
  reduction steps‚Äù.

  This is a good intuition for the usual coinductive notion of weak
  bisimilarity.  However, there is a subtle difference between
  coinductive bisimilarity and the guarded definition we employ for
  the free error domain. It is true that the "global" version of weak
  bisimilarity for the free error domain (defined using quantification
  over clocks) coincides with the usual coinductive definition for the
  Delay monad, and hence is transitive. However, this form of
  transitivity has a big limitation: since it only applies globally,
  transitive reasoning cannot be combined with guarded recursion/L√∂b
  induction. For this reason we work as much as possible with strong
  bisimilarity, which is transitive even without moving to global
  reasoning.

  The formal explanation for why weak bisimilarity is not transitive
  is given by our no-go theorem: If it were transitive, then it follows
  from the theorem that it would be the trivial relation.  We can
  spell out part of the proof of the theorem for the specific case of
  weak bisimilarity, as follows:

  Suppose ‚âà is transitive. Then we show that l ‚âà fix Œ∏ for all l.
  Indeed, by L√∂b induction we assume that this holds later, and then
  we have

     (1)          (2)                (3)
    l ‚âà Œ∏ (next l) ‚âà Œ∏ (next (fix Œ∏)) = fix Œ∏

  where:
  (1) holds because weak bisimilarity is step-insensitive.
  (2) follows by definition of ‚âà when both sides are Œ∏, which says that the
  elements under the Œ∏ are related one time step later. This is precisely our
  L√∂b-induction hypothesis.
  (3) is the unfolding of a guarded fixpoint.

  And finally by transitivity, we conclude l ‚âà fix Œ∏.  A symmetric
  argument shows fix Œ∏ ‚âà l for all l, and hence by transitivity we
  have l ‚âà l for all l.
  
* Reviewer A's Q on why adequacy applies only to closed terms

  Reviewer A asks why we formulated our adequacy theorem for closed
  terms of type ‚Ñï only, rather than giving a result for open terms of
  any type.

  We could formulate a version for open terms; it would say that
  if related closed terms are passed to our relation, then the outputs
  are related.  However, for our purposes we only need the result for
  closed terms, as is typical for adequacy results in previous work.
  

* Reviewer B's big-picture questions

  Reviewer B asks several questions about the broader relationship of
  our work to existing research on gradual typing. We address each of
  these below, and plan to incorporate these answers into the revised
  version of our paper.

** Novelty compared with New-Licata denotational model
   
  Our work takes as a starting point the denotational approach of New
  and Licata. However, in the intensional (SGDT) setting, the
  compositional theory of New-Licata breaks down: when accounting for
  steps, the casts are no longer embedding-projection pairs. Our work
  provides a novel refinement to their theory that allows for some
  compositional analysis to be recovered even in the context of
  intensional reasoning.

  Our main interest in using guarded domain theory over classical
  domain theory is in our future plans for this line of work. As
  discussed in the paper, Guarded domain theory can be used to model
  languages with higher-order store or a dynamically extensible sum
  type. These are common in gradually typed languages, and indeed some
  logical relations models for gradual typing have modeled dynamically
  extensible sum types ([1], [2]). This paper is a first step towards
  a denotational account that scales to these common features.

  The reason that classical domain theory cannot be used for these
  models is folklore and somewhat technical. While guarded domain
  theory allows for the solution of *arbitrary* guarded domain
  equations, in classical domain theory only domain equations that
  satisfy certain continuity principles can be solved. However,
  languages with higher-order store or unrestricted dynamically
  extensible sum types require solving simultaneous domain
  equations like

    World = Name ‚áÄ_fin Type
    Type  = Functor(World, Set)

  (E.g., see https://doi.org/10.48550/arXiv.1208.3596 which uses ùìü(Val) rather than Set)

  For higher-order store, Name is the names of memory cells, and for
  extensible sum it is the tag names on the dynamic type. This
  equation is solvable in guarded domain theory if we add a later to
  the occurrence of Type, but it is not solvable in classical domain
  theory because it is not continuous in Type.

** Benefits of the denotational approach for the gradual typing researcher

   Reviewer B asks about the benefits of the denotational approach for
   gradual typing researchers over the usual operational approach.
   The benefits in this setting are the same as the benefits of
   denotational semantics more broadly: Denotational methods are
   compositional and reusable, and allow for the use of existing
   mathematical constructs and theorems, e.g., partial orderings,
   monads, etc. This allows for a more structural approach where the
   term semantics is an ordinary call-by-push-value semantics.  In
   contrast, operational methods tend to require a significant amount
   of boilerplate work to be done from scratch in each new
   development.

   As a specific example of the compositional nature of our approach,
   the treatment of the cast rules in our work is more compositional
   than in previous work using operational semantics. The cast rules
   needed for the proof of graduality build in composition of type
   precision derivations. Rather than proving these from scratch, we
   are able to take as primitive simpler versions of the cast rules
   that are easier to prove in the model. Then from these simpler
   rules, we derive the original ones using compositional reasoning.

   Our denotational approach identifies reusable structure and is
   independent of the particular syntax of the language.  This makes
   it particularly straightforward to accommodate additions to the
   language: adding support for a new type amounts to defining a new
   object and extending the dynamic type accordingly. It also may be
   possible to model alternative cast semantics by changing the
   definition of the dynamic type and some type casts.

   In contrast, the operational semantics is not as readily
   extensible, generally requiring adding cases to the logical
   relations and the inductive proofs. In a mechanized metatheory this
   is a kind of "copy-paste" reusability rather than true code reuse
   as you get with a denotational model. Lastly, another benefit to
   the denotational approach is that it is trivial to establish the
   validity of the Œ≤ and Œ∑ principles, because they are equalities in
   the semantics, whereas they require tedious manual proof in the
   logical relations approach.

   Additionally, while we advocate for a denotational approach, the
   overall structure of our semantics could be applied to a logical
   relations proof. For example, there could be a step-indexed logical
   relation for strong error ordering and one for weak bisimilarity,
   corresponding to the fact that objects in our denotational model
   have an error ordering and a bisimilarity relation. Then the
   precision rules for casts could be similarly proven using
   perturbations.

** Implications for operational semantics of gradual typing?

   Reviewer B asks whether our work has implications for the variety
   of design proposals for the operational semantics of gradual typing
   and if it informs new design principles for gradually typed
   languages. While we have focused on a single semantics here, we
   think our development could be adapted to prove graduality and
   (more limited) equational reasoning for other cast semantics.

   In addition, while we have focused on proving extensional
   properties in this work, the intensional nature of our semantics
   could be viewed as a form of *cost semantics* where a runtime cost
   is incurred from inspecting the dynamic type. This could possibly
   be used to verify that cast optimizations such as space efficient
   implementations are both extensionally correct but also reduce the
   cost semantics. But this is all future work.

** What do we learn about gradual typing through this new lens?

   Although the application of our work is to semantics of gradual
   typing, the principal technical contribution is actually about how
   to reason compositionally in the step-indexed setting.  One major
   takeaway is that it is even possible to recover some amount of
   transitive reasoning in the intensional setting. The novel
   methodology of perturbations that enables us to do so may find
   applications in work applying guarded type theory to other domains.

   In addition, our work presents a new semantic model and methodology
   for mechanized metatheory of gradually-typed languages which we
   intend to extend to gradually-typed languages with advanced
   features such as effects and higher-order store.
   
* Reviewer C's comment on application of the semantics

  Reviewer C says that "The semantics developed isn't used for
  anything in the paper". However, this is not the case --- the
  type-based reasoning mentioned in the introduction is a consequence
  of the equational theory in Section 2, which we prove sound by
  defining an interpretation of the rules into our model (Theorem
  5.12, line 1002).

  The application is provided in Theorem 5.12 (line 1002)
  where we give an interpretation of the syntactic rules of Section 2,
  thus providing a proof of graduality as well as the validity of the
  equational theory.

[1]: Ahmed, Findler, Siek, Wadler.: Blame For All, POPL 2011, https://doi.org/10.1145/1925844.1926409
[2]: New, Jamner, Ahmed.: Graduality and parametricity: together again for the first time, POPL 2020 https://doi.org/10.1145/3371114
