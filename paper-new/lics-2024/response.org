We thank the reviewers for their insightful feedback which will help
us improve the paper independent of the final decision here. We
perceive from the reviews that the main criticisms of the paper are in
the writing and are confident we can address these in revisions. We
first address the most pertinent technical and writing questions, and
then more minor ones. If we don't address a question below it is
because we believe the fix to it to be relatively simple.

To summarize, the most significant changes we plan to make in the
revisions are:
1. Expand on section 4.2 to more thoroughly motivate the need for our
   extensional and intensional models, e.g., lack of transitive
   reasoning and the need to reason about perturbations
2. Introduce the concrete model earlier to help motivate the abstract
   models and constructions.
3. Provide more background on the axioms for gradual typing
4. Clarify the key steps in the construction of the intensional and
   extensional model.
5. Explicitly mention the restrictions on the use of ticks in TCTT
6. Fix the typos, and address the smaller points included below.

* Reviewer 2 re-writing
*MAX:* Eric please make this more organized into distinct points.

Reviewer 2 says that the paper is difficult to follow and feels heavily
compressed. Reviewer 1 points out that Section 4.2 is the weakest in the
paper, and also asks important questions about our notion of extensional modelm
namely:
1.  Why is the idealized notion of extensional model not suitable for our semantics?
2.  Why can't we require transitivity in our notion of extensional model?
   
We believe that the main source of confusion
in the paper as it stands now is that the notions of extensional and
intensional model as introduced in Section 4.2 and defined in Section 5 are
introduced without first motivating why the concrete construction doesn't
satisfy the definition of an idealized model.

We intended for the paper to include material explaining this but we removed
it unintentionally while editing the paper, so we include the explanation below.

In short, the justification for the lack of transitive reasoning is that
the semantic model of term precision that we use (≾) is provably
*not transitive*, and neither is the *guarded definition* of weak
bisimilarity (≈). The argument is as follows: if a relation ~ on L A is

1. A congruence wrt θ: ▶_t (x t ~ y t) ⇒ θx ~ θy
2. Right-closed under delay: x ~ y ⇒ x ~ δ y
3. Transitive

Then by Löb induction you can show every element x satisfies x ~ fix
θ.  If the relation is also left-closed then this implies for any x,y
that x ~ fix θ and fix θ ~ y and so by transitivity x ~ y and the
relation is trivial. But these relations (≾, ≈) satisfy (1) and (2)
but are non-trivial so they are not transitive.

This does not contradict that Capretta's delay monad has a transitive
notion of weak bisimilarity. Capretta's weak bisimilarity should be
equivalent to the global (i.e., ∀k) version of our guarded weak
bisimilarity. This global relation therefore should be transitive. So
we almost get out an idealized model this way, but only for
"clock-irrelevant" types, which are essentially the ones that do not
use Dyn. However we need the weaker formulation of an extensional
model in order to get a compositional construction that models Dyn,
which is obviously central to gradual typing.

Finally, as briefly mentioned (378-381) transitive reasoning is not an
essential component of graduality, for instance it was not in the
original SNAPL 2015 formulation of the gradual guarantee. It was only
added in New and Licata's work which used models that supported
transitive reasoning.


* R1 why posets?
Reviewer 1 asks why the objects in our concrete model have a notion of
ordering, as opposed to just being types along with a weak
bisimilarity relation. There are two potential interpretations of this
question. The first is that the reviewer is asking why an ordering
relation is needed on objects, to which our response is that we are
trying to model the term precision ordering of gradual typing.  In
particular, the abstract models include a notion of commutative
squares between morphisms, and the way we instantiate in the concrete
setting is by equipping the objects with a partial order which then
extends to the morphisms, i.e., we define a commuting square f ⊑ g as
"if x ≤ y then f x ≤ g y".

The second interpretation of the question about the ordering is that
the reviewer is asking why we cannot treat the ordering
"synthetically" in the same way that we treat the step-indexing
synthetically via SGDT. That is, our objects would be treated as sets
when working internally, but from an "external viewpoint" they would
be sets equipped with a partial order. This is an interesting
possibility which, as we mention in the Discussion section of the
paper, we plan to explore as future work.

* R1 why weak bisimilarity?
Reviewer 1 also asks why all of the objects in our concrete model need to come
equipped with a notion of weak bisimilarity. That is, why can't weak
bisimilarity be limited to the guarded lift monad? Tracing things back one step,
we could say that this is because the intensional model requires the vertical
hom-sets to have a notion of "weak bisimilarity", which in this abstract setting
is specified as a reflexive, symmetric relation that is closed under composition.
As was the case with the partial order discussed above, when we define the
concrete model, we need to satisfy this requirement. We do so by equipping the
*objects* with a reflexive, symmetric relation, which then extends to a relation
on the vertical hom-sets (morphisms between predomains) in the obvious way.

Of course, one could now ask, as Reviewer 1 does, why the definition of
intensional model is designed this way. The reason the definition of
intensional model requires a notion of weak bisimilarity for all vertical
hom-sets is that we need to reason about weak bisimilarity for
*higher-order* types, e.g., when we cast a function to another function,
no steps are taken at a first-order. But when the function is applied,
steps may be taken. So we need a way to lift the bisimilarity relation
to functions.

* R1 loves Agda

Reviewer 1 mentions that the submission would be stronger if the Agda
formalization was complete.

We are planning to complete at least some portions of the results here
in guarded cubical Agda, especially the adequacy
results. Unfortunately, there is still significant effort to finish
the mechanization as a faithful rendering of the constructions in our
paper would rely on formalizations of double categories and CBPV
models, which are not readily available.

On the other hand, we would like to emphasize that we co-designed the
concrete model with the formalization in Agda and this was immensely
helpful for the authors to better understand guarded type theory and
domain theory, even without a complete mechanization.

* Other

> By the way, is this a variation of double category that appeared before in the literature?

We are not aware of a name for this variation from an existing source.

> lines 720-727, you do not really say what changes in an intensional model when we give up
> horizontal composition being a strict CBPV morphism. You only say that now m_V and m_E are
> CBPV morphisms, while before they where strict. So what changes concretely?

Composition is only a lax morphism because relation constructors
generally do not commute strongly strongly with relation
composition. Specifically, ->, U and F only laxly commute with
relation composition. This is already true of the action of the
function arrow on ordinary relations, and for U and F it is because
value type relations are composed by the ordinary relation composition
whereas the computation type relations are composed by the free
comoposition defined inductively at 1034.

* ℕ in Perturbations

> Section 5.3.1, when extending F with perturbations, you have `ℕ × P_A` instead of
> simply `P_A`. Can you comment on why you have the cartesian product with natural
> numbers here? What breaks if you do not add it and simply take P_A? It is not
> immediately clear to me by quickly reading through Appendix C.1.

The need for natural numbers arises because when we downcast from the dynamic
type to the function type, we need to insert a θ because of the '▹' in the
function case of the dynamic type (see line 1191 in the paper). Thus, in order
for the representability properties to hold with for this downcast with respect
to the lock-step error ordering, we need to insert a delay (i.e., θ ∘ next) on
the other side of the inequality to keep both sides synchronized. The natural number
in a perturbation on lift A corresponds to the insertion of this delay. That is,
the interpretation of a perturbation (n, p_A) as an endomorphism on lift A is to
perturb A according to p_A (under the functorial action of lift), and then perform
a delay n-times.


* Minor Qs

> l341: Is this admissibility claim proved somewhere?

Yes, this is proven in Appendix A.
  
> l378: Can the benefit of transitivity being admissible instead of
> derivable be explained? ...

This was a bit unclearly worded: reflexivity is admissible from
congruence so we don't bother adding it as a primitive. Transitivity
is *not* admissible, and this is desired as our extensional models
(Section 5.1) do not model transitivity.

> l388: The point about upcasts and downcasts being some kinds of
> least upper bounds and greatest lower bounds is interesting ... Is
> it just that these are basically adjoints or some kind?

Yes, the technical content of this intuition is that they are Galois
connections, which can be formulated as saying they are given
pointwise by a certain join/meet.

Reviewer 2 asks about the details of categories internal to CBPV with
strict/lax morphisms.

For models using strict morphisms, we know of two ways to answer:
1. Abstractly, CBPV can be defined as an EAT or a GAT and so the
   category of models with strict morphisms is complete.
2. Concretely, the pullback of two CBPV models is given by taking the
   pullback of the component categories.

For the lax morphisms, we interpret this definition using the concrete
construction of pullbacks by taking the pullbacks of the component
categories and simply weakening the morphisms to have lax preservation
of connectives rather than strong. This might be abstractly
characterized by consider CBPV as a 2-monad on the 2-category of pairs
of categories and considering strict and lax morphisms, but we have
not pursued this in detail.
   
> l551: Is Definition 4.2 a universal property for ‘f’, or can there
> be more than one representative?

Definition 4.2 characterizes f up to order equivalence.

> l603: It occurred to me that maybe virtual double categories could
> be a useful intermediate abstraction here? I am curious if you
> thought about that.

We had considered this but didn't have any use for the more general
2-cells in modeling graduality so we used the simpler reflexive graph
categories.

* 1-Topos of Trees

Reviewer 2 points out that our weak bisimilarity notion is trivial in
the topos of trees model, but not the (∞, 1)-topos of trees model.
Reviewer 3 asks a related question of the difference between using
TCTT and CTT.

This is a fair point of discussion that we hadn't previously
considered and we appreciate the reviewer pointing it out. Here is an
overview of our understanding of this point:

1. Our treatment was based on the Greatest HITs paper/TCTT which uses
   the (∞,1)-topos of trees as a model where our definitions are
   non-trivial, but proving this relies on induction under clocks,
   which is not available in other guarded type theories such as
   CTT. The question then is if our development could be modified to
   be more portable to other guarded type theories/models such as CTT.

2. In the 1-topos of trees model our definition of weak bisimilarity
   and our closure of lock-step error ordering under weak bisimilarity
   are trivial due to the combination of relating terms with differing
   numbers of steps and using propositional truncation. Note that this
   is a distinct issue from the one mentioned earlier in the response
   about transitivity being incompatible with weak bisimilarity.

   However, the lock-step error ordering and the free composition of
   computation relations don't allow differing numbers of steps so the
   argument fails and indeed these have non-trivial semantics even in
   the 1-topos model. So there is no issue with using posets for
   strong ordering, only in forcing notions using weak bisimilarity to
   be propositions.

3. We have studied this issue since the reviews and we are confident
   that allowing weak bisimilarity to be a Set rather than a Prop
   would not impact any of the main results of the paper, as we only
   use (guarded) weak bisimilarity as an input to the adequacy
   theorem, whose output is a Prop and so no result can depend on the
   details of which bisimilarity proof is constructed. This change
   would allow us to remove dependence on induction under clocks from
   Greatest HITs in the paper and corresponding axioms in our ongoing
   Agda formalization.
